Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Liu2012,
abstract = {This thesis examines the application of statistical signal processing approaches to data arising from surveys intended to measure phychological and sociological phenomena underpinning human social dynamics. The use of signal processing methods for analysis of signals arising from measurement of social, biological, and other non-traditional phenomena has been an important and growing area of signal processing research over the past decade. Here, we explore the application of statistical modeling and signal processing concepts to data obtained from the Global Group Relations Project, specifically to understand and quantify the effects and interactions of social psychological factors related to intergroup conflicts. We use Bayesian networks to specify prospective models of conditional dependence. Bayesian networks are determined between social psychological factors and conflict variables, and modeled by directed acyclic graphs, while the significant interactions are modeled as conditional probabilities. Since the data are sparse and multi-dimensional, we regress Gaussian mixture models (GMMs) against the data to estimate the conditional probabilities of interest. The parameters of GMMs are estimated using the expectation-maximization (EM) algorithm. However, the EM algorithm may suffer from over-fitting problem due to the high dimensionality and limited observations entailed in this data set. Therefore, the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) are used for GMM order estimation. To assist intuitive understanding of the interactions of social variables and the intergroup conflicts, we introduce a color-based visualization scheme. In this scheme, the intensities of colors are proportional to the conditional probabilities observed.},
author = {Liu, Hui},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayesian Networks and Gaussian Mixture Models.pdf:pdf},
number = {2},
pages = {81--87},
title = {{Bayesian Networks and Gaussian Mixture Models in Multi-Dimensional Data Analysis with Application to Religion-Conflict Data}},
year = {2012}
}
@article{Lecture2012,
author = {Lecture, Erik Sudderth and Curves, R O C and Ml, Gaussian},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayes Decision Rule.pdf:pdf},
title = {{Introduction to Machine Learning Generative Classifiers}},
year = {2012}
}
@article{Ma2019,
author = {Ma, Tengyu and Ng, Andrew},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/The EM Algorithm (Stanford).pdf:pdf},
pages = {1--14},
title = {{CS229 Lecture notes The EM algorithm}},
volume = {1},
year = {2019}
}
@article{Yule1909,
author = {Yule, G. Udny},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/The Applications of the Method fo Correlation to Social and Economic Statistics.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {4},
pages = {721--730},
title = {{The Applications of the Method of Correlation to Social and Economic Statistics Author ( s ): G . Udny Yule Source : Journal of the Royal Statistical Society , Vol . 72 , No . 4 ( Dec ., 1909 ), pp . 721-730 Published by : Blackwell Publishing for the Roy}},
volume = {72},
year = {1909}
}
@article{brown1961,
abstract = {Exponential smoothing is a formalization of the familiar learning process, which is a practical basis for statistical forecasting. Higher orders of smoothing are defined by the operator St n(x) =$\alpha$ St n-1(x)+(1- $\alpha$ ) St-1 n(x), where St 0(x) = xt, $0<\alpha<1$. If one assumes that the time series of observations {xt} is of the form xt= nt+$\Sigma$ i=0 i=N ai ti, where nt is a sample from some error population, then least squares estimates of the coefficients ai can be obtained from linear combinations of the operators S, S2, ⋯ , SN+1. Explicit forms of the forecasting equations are given for N = 0, 1, and 2. This result makes it practical to use higher order polynomials as forecasting models, since the smoothing computations are very simple, and only a minimum of historical statistics need be retained in the file from one forecast to the next.},
author = {Brown, Robert G and Meyer, Richard F and D'Esopo, D A},
issn = {0030364X, 15265463},
journal = {Operations Research},
number = {5},
pages = {673--687},
publisher = {INFORMS},
title = {{The Fundamental Theorem of Exponential Smoothing}},
url = {http://www.jstor.org/stable/166814},
volume = {9},
year = {1961}
}
@article{Paalanen2004,
abstract = {The main purpose of this project was to develop a Bayesian classification toolbox with Gaussian mixture probability densities for Matlab. The toolbox has two parts: the training part estimates Gaussian mixture parameters from training samples and the classifier part classifies new samples according to the estimated parameters. Three different expectation maximization algorithms for Gaussian mixture estimation were implemented: the basic expectation maximization algorithm, an algorithm proposed by Figueiredo and Jain, and the greedy expectation maximization algorithm by Verbeek, Vlassis and Kr{\"{o}}se. The toolbox was tested with neighbor-bank transformed forest spectral color data, wave- forms and noise data, and letter image recognition data. The basic expectation maximiza- tion algorithm produces the best classification accuracy, but requires tuning parameters by hand and is failure prone. Figueiredo-Jain algorithm produces better accuracy than the greedy algorithm, but requires a certain number of training data before itworks at all. The greedy algorithm has much lower requirement for the amount of training data.},
author = {Paalanen, Pekka},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayesian Classification using Gaussian Mixture Models.pdf:pdf},
keywords = {bayesian classification,expectation maximization,gaussian mixture,gmmbayes toolbox,maximum likelihood,overfitting},
pages = {1--35},
title = {{Bayesian Classification Using Gaussian Mixture Model and EM Estimation}},
url = {http://www2.it.lut.fi/project/gmmbayes/downloads/doc/},
year = {2004}
}
@article{Ng2000,
abstract = {The EM algorithm},
author = {Ng, Andrew},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/cs229-notes8.pdf:pdf},
journal = {CS229 Lecture notes},
number = {X},
pages = {1--8},
title = {{9 - The EM algorithm}},
volume = {1},
year = {2000}
}
@article{McNicholas2008,
abstract = {Parsimonious Gaussian mixture models are developed using a latent Gaussian model which is closely related to the factor analysis model. These models provide a unified modeling framework which includes the mixtures of probabilistic principal component analyzers and mixtures of factor of analyzers models as special cases. In particular, a class of eight parsimonious Gaussian mixture models which are based on the mixtures of factor analyzers model are introduced and the maximum likelihood estimates for the parameters in these models are found using an AECM algorithm. The class of models includes parsimonious models that have not previously been developed. These models are applied to the analysis of chemical and physical properties of Italian wines and the chemical properties of coffee; the models are shown to give excellent clustering performance. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
author = {McNicholas, Paul David and Murphy, Thomas Brendan},
doi = {10.1007/s11222-008-9056-0},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Parsimonius Gaussian Mixture Models.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Cluster analysis,Factor analysis,Mixture models,Model-based clustering,Probabilistic principal components analysis},
number = {3},
pages = {285--296},
title = {{Parsimonious Gaussian mixture models}},
volume = {18},
year = {2008}
}
@article{Sridharan2017,
author = {Sridharan, R.},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/gmm-em.pdf:pdf},
pages = {11},
title = {{Gaussian mixture models and the EM Algorithm (Lecture Notes - MIT CSAIL)}},
url = {https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf},
year = {2017}
}
@misc{Geller2009,
author = {Geller, Martinne},
booktitle = {Reuters},
title = {{UPDATE 2-Wal-Mart Dec sales disappoint, Target beats | Reuters}},
url = {https://www.reuters.com/article/usa-retailsales-walmart/update-2-wal-mart-dec-sales-disappoint-target-beats-idUKN0835090420090108},
urldate = {2021-04-14},
year = {2009}
}
@article{Li2019,
abstract = {The rapid acceleration of microbial genome sequencing increases opportunities to understand bacterial gene function. Unfortunately, only a small proportion of genes have been studied. Recently, TnSeq has been proposed as a cost-effective, highly reliable approach to predict gene functions as a response to changes in a cell's fitness before-after genomic changes. However, major questions remain about how to best determine whether an observed quantitative change in fitness represents a meaningful change. To address the limitation, we develop a Gaussian mixture model framework for classifying gene function from TnSeq experiments. In order to implement the mixture model, we present the Expectation-Maximization algorithm and a hierarchical Bayesian model sampled using Stan's Hamiltonian Monte-Carlo sampler. We compare these implementations against the frequentist method used in current TnSeq literature. From simulations and real data produced by E.coli TnSeq experiments, we show that the Bayesian implementation of the Gaussian mixture framework provides the most consistent classification results.},
author = {Li, Kevin and Chen, Rachel and Lindsey, William and Best, Aaron and DeJongh, Matthew and Henry, Christopher and Tintle, Nathan},
doi = {10.1142/9789813279827_0016},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Implementing and Evaluating a Gaussian Mixture Framework.pdf:pdf},
issn = {23356936},
journal = {Pacific Symposium on Biocomputing},
keywords = {Bacteria,Bayesian,Genetics},
number = {2019},
pages = {172--183},
pmid = {30864320},
title = {{Implementing and evaluating a Gaussian mixture framework for identifying gene function from TnSeq data}},
volume = {24},
year = {2019}
}
@misc{Kaggle2014,
author = {Kaggle},
title = {{Walmart Recruiting - Store Sales Forecasting by Kaggle}},
url = {https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data},
year = {2014}
}
@article{Stone1962,
author = {Stone, M. and Weiss, Lionel},
doi = {10.2307/2333997},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Pun-Statistical-Decision-Theory.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
number = {3/4},
pages = {577},
title = {{Statistical Decision Theory.}},
volume = {49},
year = {1962}
}
@article{Models1998,
abstract = {We describe the maximum-likelihood parameter estimation problem and howthe Expectation- Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EMalgorithm as it is often given in the literature. We then develop the EMpa- rameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any conver- gence properties. We try to emphasize intuition rather than mathematical rigor.},
author = {Models, Hidden Markov},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models.pdf:pdf},
isbn = {0226775429},
issn = {0042-0980},
journal = {ReCALL},
number = {510},
pages = {126},
title = {{International computer science institute 1947}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.4498&rep=rep1&type=pdf},
volume = {4},
year = {1998}
}
@article{Collins2012,
author = {Collins, Michael},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Naive Bayes and Gaussian Models for Classification.pdf:pdf},
title = {{Naive Bayes and Gaussian models for classification}},
year = {2012}
}
@article{Liang2015,
abstract = {These lecture notes will be updated periodically as the course goes on. Please let us know if you find typos. Section A.1 describes the basic notation and definitions; Section A.2 describes some basic theorems.},
author = {Liang, Percy},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical Learning Theory (Winter 2015).pdf:pdf},
number = {Winter},
pages = {1--192},
title = {{CS229T/STAT231: Statistical Learning Theory (Winter 2015)}},
url = {papers3://publication/uuid/7D5405C2-EE27-4CE3-9136-35C25CC09BFE},
year = {2015}
}
@book{Akinkunmi2019,
abstract = {Introduction to Statistics Using R is organized into 13 major chapters. Each chapter is broken down into many digestible subsections in order to explore the objectives of the book. There are many real-life practical examples in this book and each of the examples is written in R codes to acquaint the readers with some statistical methods while simultaneously learning R scripts.},
author = {Akinkunmi, Mustapha},
booktitle = {Synthesis Lectures on Mathematics and Statistics},
doi = {10.2200/S00899ED1V01Y201902MAS024},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Introduction to Statistical Learning.pdf:pdf},
isbn = {9781681735054},
issn = {19381751},
keywords = {confidence interval,correlation analysis,descriptive statistics,hypothesis testing,probability distributions,regression analysis,sampling distribution},
number = {4},
pages = {1--235},
title = {{Introduction to Statistics Using R}},
volume = {11},
year = {2019}
}
@article{Shen2012,
abstract = {Background subtraction is often the first step of many computer vision applications. For a background subtraction method to be useful in embedded camera networks, it must be both accurate and computationally efficient because of the resource constraints on embedded platforms. This makes many traditional background subtraction algorithms unsuitable for embedded platforms because they use complex statistical models to handle subtle illumination changes. These models make them accurate but the computational requirement of these complex models is often too high for embedded platforms. In this paper, we propose a new background subtraction method which is both accurate and computational efficient. The key idea is to use compressive sensing to reduce the dimensionality of the data while retaining most of the information. By using multiple datasets, we show that the accuracy of our proposed background subtraction method is comparable to that of the traditional background subtraction methods. Moreover, real implementation on an embedded camera platform shows that our proposed method is at least 5 times faster, and consumes significantly less energy and memory resources than the conventional approaches. Finally, we demonstrated the feasibility of the proposed method by the implementation and evaluation of an end-to-end real-time embedded camera network target tracking application. Copyright {\textcopyright} 2012 ACM.},
author = {Shen, Yiran and Hu, Wen and Liu, Junbin and Yang, Mingrui and Wei, Bo and Chou, Chun Tung},
doi = {10.1145/2426656.2426686},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Efficient Backgrou9nd Subtraction for Real Time Tracking.pdf:pdf},
isbn = {9781450311694},
journal = {SenSys 2012 - Proceedings of the 10th ACM Conference on Embedded Networked Sensor Systems},
keywords = {Background subtraction,Compressive sensing,Embedded camera networks,Gaussian mixture models,Object tracking},
pages = {295--308},
title = {{Efficient background subtraction for real-time tracking in embedded camera networks}},
year = {2012}
}
@article{Kelly2009,
author = {Kelly, Anthony},
doi = {10.1017/cbo9780511609992.012},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayes Theorem Proof.pdf:pdf},
journal = {Decision Making Using Game Theory},
number = {1},
pages = {190--191},
title = {{Proof of Bayes's theorem}},
year = {2009}
}
@article{Zuffi2010,
author = {Zuffi, Silvia},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A view of the EM Algorithm that Justifies Incremental, Sparse, and Other Variants.pdf:pdf},
journal = {Learning in graphical models},
title = {{(tutorial) A view of the EM algorithm and Bayesian networks and Gibbs sampling tutorial}},
year = {2010}
}
@article{Bapna2017,
abstract = {The growing importance of online social networks provides fertile ground for researchers seeking to gain a deeper understanding of fundamental constructs of human behavior, such as trust and forgiveness, and their linkage to social ties. Through a field experiment that uses data from the Facebook API to measure social ties that connect our subjects, we separate forward-looking instrumental trust from static intrinsic trust and show that the level of instrumental trust and forgiveness, and the effect of forgiveness on deterring future defections, crucially depend on the strength of social ties. We find that the level of trust under social repeated play is greater than the level of trust under anonymous repeated play, which in turn is greater than the level of trust under anonymous one shot games. We also uncover forgiveness as a key mechanism that facilitates the cooperative equilibrium being more stable in the presence of social ties: If the trading partners are socially connected, the equilibrium is more likely to return to the original cooperative one after small disturbances.},
author = {Bapna, Ravi and Qiu, Liangfei and Rice, Sarah},
doi = {10.25300/MISQ/2017/41.3.08},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A Tutorial on HIdden Markov Models.pdf:pdf},
issn = {21629730},
journal = {MIS Quarterly: Management Information Systems},
keywords = {Forgiveness,Repeated games,Social ties,Trust},
number = {3},
pages = {841--866},
title = {{Repeated interactions versus social ties: Quantifying the economic value of trust, forgiveness, and reputation using a field experiment}},
volume = {41},
year = {2017}
}
@article{Smyth2019,
abstract = {Course CS274 Notes},
author = {Smyth, Padhraic},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/The EM Algorithm for Gaussian Mixtures.pdf:pdf},
journal = {Lecture Notes on Course CS274A: Probabilistic Learning: Theory and Algorithms, WInter 2019},
pages = {1--3},
title = {{The EM Algorithm for Gaussian Mixtures Finite Mixture Models}},
url = {https://www.ics.uci.edu/$\sim$smyth/courses/cs274/notes/EMnotes.pdf},
year = {2019}
}
@book{box1976time,
author = {Box, G E P and Jenkins, G M and Day, H},
isbn = {9780816211043},
publisher = {Holden-Day},
series = {Holden-Day series in time series analysis and digital processing},
title = {{Time Series Analysis: Forecasting and Control}},
url = {https://books.google.com/books?id=1WVHAAAAMAAJ},
year = {1976}
}
@article{Tibshirani,
author = {Tibshirani, Ryan},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Coordinate Descent (CMU).pdf:pdf},
pages = {1--37},
title = {{Coordinate Descent Last time : dual methods and ADMM}}
}
@article{Bishop2006,
abstract = {If we define a joint distribution over observed and latent variables, the correspond-ing distribution of the observed variables alone is obtained by marginalization. This allows relatively complex marginal distributions over observed variables to be ex-pressed in terms of more tractable joint distributions over the expanded space of observed and latent variables. The introduction of latent variables thereby allows complicated distributions to be formed from simpler components. In this chapter, we shall see that mixture distributions, such as the Gaussian mixture discussed in Section 2.3.9, can be interpreted in terms of discrete latent variables. Continuous latent variables will form the subject of Chapter 12. As well as providing a framework for building more complex probability dis-tributions, mixture models can also be used to cluster data. We therefore begin our discussion of mixture distributions by considering the problem of finding clusters in a set of data points, which we approach first using a nonprobabilistic technique called the K-means algorithm (Lloyd, 1982). Then we introduce the latent variable Section 9.1 423 424 9. MIXTURE MODELS AND EM view of mixture distributions in which the discrete latent variables can be interpreted as defining assignments of data points to specific components of the mixture. A gen-Section 9.2 eral technique for finding maximum likelihood estimators in latent variable models is the expectation-maximization (EM) algorithm. We first of all use the Gaussian mixture distribution to motivate the EM algorithm in a fairly informal way, and then we give a more careful treatment based on the latent variable viewpoint. We shall Section 9.3 see that the K-means algorithm corresponds to a particular nonprobabilistic limit of EM applied to mixtures of Gaussians. Finally, we discuss EM in some generality. Section 9.4 Gaussian mixture models are widely used in data mining, pattern recognition, machine learning, and statistical analysis. In many applications, their parameters are determined by maximum likelihood, typically using the EM algorithm. However, as we shall see there are some significant limitations to the maximum likelihood ap-proach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be given using the framework of variational inference. This requires little additional computation compared with EM, and it resolves the principal difficulties of maxi-mum likelihood while also allowing the number of components in the mixture to be inferred automatically from the data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02626v3},
author = {Bishop, Christopher M.},
eprint = {arXiv:1506.02626v3},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/GMM and Expectation Maximization.pdf:pdf},
isbn = {9780387310732},
journal = {Pattern Recognition and Machine Learning},
pages = {423--459},
title = {{Mixture Models and EM}},
url = {http://books.google.pt/books?id=kTNoQgAACAAJ},
year = {2006}
}
@article{Srivastava2007,
abstract = {1 Learning goals • Know what generative process is assumed in a mixture model, and what sort of data it is intended to model • Be able to perform posterior inference in a mixture model, in particular-compute the posterior distribution over the latent variable-compute the posterior predictive distribution • Be able to learn the parameters of a mixture model using the Expectation-Maximization (E-M) algorithm 2 Unsupervised learning So far in this course, we've focused on supervised learning, where we assumed we had a set of training examples labeled with the correct output of the algorithm. We're going to shift focus now to unsupervised learning, where we don't know the right answer ahead of time. Instead, we have a collection of data, and we want to find interesting patterns in the data. Here are some examples: • The neural probabilistic language model you implemented for Assignment 1 was a good example of unsupervised learning for two reasons:-One of the goals was to model the distribution over sentences, so that you could tell how "good" a sentence is based on its probability. This is unsupervised, since the goal is to model a distribution rather than to predict a target. This distribution might be used in the context of a speech recognition system, where you want to combine the evidence (the acoustic signal) with a prior over sentences.-The model learned word embeddings, which you could later analyze and visualize. The embeddings weren't given as a supervised signal to the algorithm, i.e. you didn't specify by hand which vector each word should correspond to, or which words should be close together in the embedding. Instead, this was learned directly from the data.},
author = {Srivastava, Roger Grosse and Nitish},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/mixture_models.pdf:pdf},
journal = {The Journal o f International Trade and Diplomacy},
keywords = {9TR},
number = {2},
pages = {159--202},
title = {{Lecture 16: Mixture models}},
url = {http://link.springer.com/10.1007/978-3-642-19318-7_53},
volume = {1},
year = {2007}
}
@article{Minimization2016,
author = {Minimization, Coordinate},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/IE598-lecture12-coordinate descent algorithms.pdf:pdf},
pages = {1--7},
title = {{Lecture 12 : Coordinate Descent Algorithms Introduction : Coordinate Descent Algorithms}},
year = {2016}
}
@article{Liang2012,
author = {Liang, Dawen},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/em_details.pdf:pdf},
pages = {1--8},
title = {{Technical Details about the Expectation Maximization (EM) Algorithm}},
year = {2012}
}
@article{Kambhatla1995,
author = {Kambhatla, N and Leen, T K},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/classifying-with-gaussian-mixtures-and-clusters.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 7},
number = {January},
pages = {681--688},
title = {{Classifying with Gaussian mixtures and clusters}},
year = {1995}
}
@article{Hh2014,
author = {Hh, T and Ht, T},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Conditional Probability, Independence, and Bayes' Theorem (MIT 18.05).pdf:pdf},
pages = {1--11},
title = {{Conditional Probability , Independence and Bayes ' Theorem Jeremy Orloff and Jonathan Bloom}},
year = {2014}
}
@article{Xing2014,
author = {Xing, Eric},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/EM Algorithm - Probabilistic Graphical Models (CMU).pdf:pdf},
pages = {2005--2009},
title = {{Probabilistic Graphical Models Learning Partially Observed GM : the Expectation - Maximization algorithm Partially observed GMs Partially observed GM}},
year = {2014}
}
@article{1392a,
abstract = {يكي از اعتقادات مهم مسيحيان، اعتقاد به گناه ذاتي است. براساس اين اعتقاد، حضرت آدم(ع) از ميوة شجرة معرفت نيك و بد خورد، در‌حالي‌كه خداوند او را از خوردن آن نهي كرده بود. اين عمل و گناه، نه‌تنها موجب سقوط آدم(ع) گرديد، بلكه همة ذرية او را نيز آلوده كرد. آگوستين و پلاگيوس دو تن از دانشمندان بزرگ مسيحيت و از مهم‌ترين نظريه‌پردازان در زمينة گناه ذاتي هستند كه تقابل ديدگاه‌هاي آنان در اين زمينه مي‌تواند بسيار جالب توجه باشد. اين مقاله با رويكرد تحليل محتوا،‌ بررسي اين آموزه را بر‌اساس ديدگاه آگوستين و پلاگيوس بر عهده دارد.},
author = {حسینی, سید مصطفی and {حسینی قلعه بهمن}, سید اکبر},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/hastie94discriminant.pdf:pdf},
journal = {معرفت ادیان},
keywords = {گناه ذاتي، سقوط، حضرت آدم(ع)، آگوستين، پلاگيوس.},
number = {3},
pages = {57--71},
title = {{No Titleچالش گناه ذاتي از نگاه آگوستين و پلاگيوس}},
url = {http://marefateadyan.nashriyat.ir/node/150},
volume = {4},
year = {1392}
}
@article{Goldberger2005,
abstract = {In this paper we propose an efficient algorithm for reducing a large mixture of Gaussians into a smaller mixture while still preserv- ing the component structure of the original model; this is achieved by clustering (grouping) the components. The method minimizes a new, easily computed distance measure between two Gaussian mixtures that can be motivated from a suitable stochastic model and the iterations of the algorithm use only the model parameters, avoiding the need for explicit resampling of datapoints. We demon- strate the method by performing hierarchical clustering of scenery images and handwritten digits.},
author = {Goldberger, Jacob and Roweis, Sam},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/NIPS-2004-hierarchical-clustering-of-a-mixture-model-Paper.pdf:pdf},
isbn = {0262195348},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Hierarchical clustering of a mixture model}},
year = {2005}
}
@article{Zhang2013,
author = {Zhang, Hao Helen},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical-Learning-Theory Lecture 2.pdf:pdf},
number = {Part I},
title = {{Lecture 2 : Statistical Decision Theory (Part I : Statistics Decision Theory)}},
year = {2013}
}
@article{Paradigm2006,
author = {Paradigm, Supervised Learning},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical-Learning-Theory, Least Squares and Bias Variance.pdf:pdf},
number = {x},
pages = {2--4},
title = {{Statistical Decision Theory , Least Squares , and Bias Variance Tradeoff Bias - Variance Tradeoff Let us consider some learning algorithm and its expected prediction error :}},
year = {2006}
}
@article{Shi2016,
abstract = {This monograph presents a class of algorithms called coordinate descent algorithms for mathematicians, statisticians, and engineers outside the field of optimization. This particular class of algorithms has recently gained popularity due to their effectiveness in solving large-scale optimization problems in machine learning, compressed sensing, image processing, and computational statistics. Coordinate descent algorithms solve optimization problems by successively minimizing along each coordinate or coordinate hyperplane, which is ideal for parallelized and distributed computing. Avoiding detailed technicalities and proofs, this monograph gives relevant theory and examples for practitioners to effectively apply coordinate descent to modern problems in data science and engineering.},
archivePrefix = {arXiv},
arxivId = {1610.00040},
author = {Shi, Hao-Jun Michael and Tu, Shenyinying and Xu, Yangyang and Yin, Wotao},
eprint = {1610.00040},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A Primer on Coordinate Descent Algorithms.pdf:pdf},
title = {{A Primer on Coordinate Descent Algorithms}},
url = {http://arxiv.org/abs/1610.00040},
year = {2016}
}
@article{Melnykov2010,
abstract = {Finite mixture models have a long history in statistics, having been used to model population heterogeneity, generalize distributional assumptions, and lately, for providing a convenient yet formal framework for clustering and classification. This paper provides a detailed review into mixture models and model-based clustering. Recent trends as well as open problems in the area are also discussed.},
author = {Melnykov, Volodymyr and Maitra, Ranjan},
doi = {10.1214/09-SS053},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/09-SS053.pdf:pdf},
issn = {19357516},
journal = {Statistics Surveys},
keywords = {Diagnostics,EM algorithm,Magnitude magnetic resonance images,Model selection,Proteomics,Textmining,Two-dimensional gel electrophoresis data,Variable selec-tion},
pages = {80--116},
title = {{Finite mixture models and model-based clustering}},
volume = {4},
year = {2010}
}
@article{Srihari,
author = {Srihari, Sargur},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Ch9.2-MixturesofGaussians.pdf:pdf},
title = {{Mixtures of Gaussians 9 . Mixture Models and EM}}
}
@article{Svensen2005,
abstract = {Bayesian approaches to density estimation and clustering using mixture distributions allow the automatic determination of the number of components in the mixture. Previous treatments have focussed on mixtures having Gaussian components, but these are well known to be sensitive to outliers, which can lead to excessive sensitivity to small numbers of data points and consequent over-estimates of the number of components. In this paper we develop a Bayesian approach to mixture modelling based on Student-t distributions, which are heavier tailed than Gaussians and hence more robust. By expressing the Student-t distribution as a marginalization over additional latent variables we are able to derive a tractable variational inference algorithm for this model, which includes Gaussian mixtures as a special case. Results on a variety of real data sets demonstrate the improved robustness of our approach. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Svens{\'{e}}n, Markus and Bishop, Christopher M.},
doi = {10.1016/j.neucom.2004.11.018},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Robust Bayesian Mixture  Modelling.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Latent variable model,Model selection,Outliers,Student-t distribution,Variational inference},
number = {1-4 SPEC. ISS.},
pages = {235--252},
title = {{Robust Bayesian mixture modelling}},
volume = {64},
year = {2005}
}
@article{Moineddin2003,
author = {Moineddin, Rahim and Upshur, Ross E G and Crighton, Eric and Mamdani, Muhammad},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Autoregression as a means of assessing the strength of seasonality.pdf:pdf},
pages = {1--7},
title = {{in a Time Series}},
volume = {7},
year = {2003}
}
@article{Wu2008a,
abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known ℓ 2 algorithm is based on cyclic coordinate descent. Our new ℓ 1 algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary ℓ 1 regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to ℓ 2 regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the ℓ 2 algorithm converges to the minimum value of the objective function. {\textcopyright} Institute of Mathematical Statistics.},
author = {Wu, Tong Tong and Lange, Kenneth},
doi = {10.1214/07-AOAS147},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Coordinate Descent for Lasso Penalized Regression.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Consistency,Convergence,Cyclic,Edgeworth's algorithm,Greedy,Model selection},
number = {1},
pages = {224--244},
title = {{Coordinate descent algorithms for lasso penalized regression}},
volume = {2},
year = {2008}
}
@article{Murray2001,
author = {Murray, Iain},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/mixture_models_EM.pdf:pdf},
number = {1},
pages = {1--6},
title = {{18 Mixture models and the EM algorithm}},
year = {2001}
}
@article{Kapoor2005,
abstract = {This paper describes a unified approach, based on Gaussian Processes, for achieving sensor fusion under the problematic conditions of missing channels and noisy labels. Under the proposed approach, Gaussian Processes generate separate class labels corresponding to each individual modality. The final classification is based upon a hidden random variable, which probabilistically combines the sensors. Given both labeled and test data, the inference on unknown variables, parameters and class labels for the test data is performed using the variational bound and Expectation Propagation. We apply this method to the challenge of classifying a student's interest level using observations from the face and postures, together with information from the task the students are performing. Classification with the proposed new approach achieves accuracy of over 83%, significantly outperforming the classification using individual modalities and other common classifier combination schemes. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Kapoor, Ashish and Ahn, Hyungil and Picard, Rosalind W.},
doi = {10.1007/11494683_9},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Mixture of Gaussian Processes for Combining Multiple Distributions.pdf:pdf},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {86--96},
title = {{Mixture of Gaussian Processes for combining multiple modalities}},
volume = {3541},
year = {2005}
}
@article{1392,
abstract = {يكي از اعتقادات مهم مسيحيان، اعتقاد به گناه ذاتي است. براساس اين اعتقاد، حضرت آدم(ع) از ميوة شجرة معرفت نيك و بد خورد، در‌حالي‌كه خداوند او را از خوردن آن نهي كرده بود. اين عمل و گناه، نه‌تنها موجب سقوط آدم(ع) گرديد، بلكه همة ذرية او را نيز آلوده كرد. آگوستين و پلاگيوس دو تن از دانشمندان بزرگ مسيحيت و از مهم‌ترين نظريه‌پردازان در زمينة گناه ذاتي هستند كه تقابل ديدگاه‌هاي آنان در اين زمينه مي‌تواند بسيار جالب توجه باشد. اين مقاله با رويكرد تحليل محتوا،‌ بررسي اين آموزه را بر‌اساس ديدگاه آگوستين و پلاگيوس بر عهده دارد.},
author = {حسینی, سید مصطفی and {حسینی قلعه بهمن}, سید اکبر},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Elements of Statistical Learning Solution Manual.pdf:pdf},
journal = {معرفت ادیان},
keywords = {گناه ذاتي، سقوط، حضرت آدم(ع)، آگوستين، پلاگيوس.},
number = {3},
pages = {57--71},
title = {{No Titleچالش گناه ذاتي از نگاه آگوستين و پلاگيوس}},
url = {http://marefateadyan.nashriyat.ir/node/150},
volume = {4},
year = {1392}
}
@article{Vapnik1992,
abstract = {Learning is posed a a problem of function estimation, for which two\nprinciples of solution are considered: empirical risk mimization\nand structural ris minimization. These two principles are applied\nto two different statements of the function estimation problem: global\nand ocal. Systematic improvements in prediction power are illustrated\nin application to zip-code recognition.},
author = {Vapnik, Vladimir},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/506-principles-of-risk-minimization-for-learning-theory.pdf:pdf},
isbn = {1-55860-222-4},
journal = {Advances in neural information processing systems},
pages = {831--838},
title = {{Principles of risk minimization for learning theory}},
url = {http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory},
year = {1992}
}
@article{Ng2005,
author = {Ng, Andrew},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Gaussian Mixture Models (Stanford).pdf:pdf},
number = {0},
pages = {1--14},
title = {{CS229 Lecture notes 2}},
year = {2005}
}
@article{Han2014,
abstract = {In order to improve the stability of an electric vehicle on different road conditions, this paper puts forward a fuzzy PI direct torque control of an adaptive, through the electronic differential control, the parameters of two high torque motor is adjusted dynamically, so as to replace the traditional mechanical differential. Analysis for system stability, the simulation results show that, with Matlab/Simulink, in a different way, this method can obtain good steady-state tracking accuracy and small dynamic error integral. {\textcopyright} (2014) Trans Tech Publications, Switzerland.},
author = {Han, Ya Jun and Du, De Yin and Chen, Bao Fan},
doi = {10.4028/www.scientific.net/AMR.953-954.1359},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Introduction to Statistical Learning Theory.pdf:pdf},
isbn = {9783038351344},
issn = {16628985},
journal = {Advanced Materials Research},
keywords = {Adaptive fuzzy PI,Direct torque control,Electric vehicle PI,Fuzzy controller},
pages = {1359--1362},
title = {{Based on adaptive fuzzy PI DTC of double wheeled electric vehicle drive control}},
volume = {953-954},
year = {2014}
}
@article{Borman2004,
abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
author = {Borman, Sean},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/EM_algorithm-1.pdf:pdf},
journal = {Unpublished paper available at http://www.seanborman.com/publications},
pages = {1--8},
title = {{The expectation maximization algorithm: A short tutorial}},
year = {2004}
}
@article{Rittscher,
author = {Rittscher, Jens and Stewart, Chuck},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Gaussian Mixture Models and the EM Algorithm.pdf:pdf},
number = {2},
pages = {1--7},
title = {{Statistical and Learning Techniques in Computer Vision Lecture 4 : Gaussian Mixture Models and the EM Algorithm Mixture models}}
}
@article{Vapnik1999,
abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs). {\textcopyright} 1999 IEEE.},
author = {Vapnik, Vladimir N.},
doi = {10.1109/72.788640},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/An Overview of Statistical Learning Theory.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {5},
pages = {988--999},
pmid = {18252602},
title = {{An overview of statistical learning theory}},
volume = {10},
year = {1999}
}
@article{Donaldson2010,
author = {Donaldson, Islay M.},
doi = {10.5465/ambpp.1969.4980479},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical Decision Theory (Columbia).pdf:pdf},
issn = {0307661X},
journal = {TLS - The Times Literary Supplement},
number = {5593},
pages = {6},
title = {{Decision theory}},
year = {2010}
}
@article{Kesumah2019,
author = {Kesumah, Fajrin Satria Dwi and Azhar, Rialdi and Russel, Edwin},
doi = {10.31227/osf.io/x9v6z},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Application of Short-term Forecasting Models for Energy Entity Stock Price - Azhar, R - Kesumah, FSD, Russel, E, Wisnu.pdf:pdf},
keywords = {ARCH Effect,GARCH Model,Investment Decision,Share Price Forecasting,Volatility},
number = {2017},
title = {{Application of Short-term Forecasting Models for Energy Entity Stock Price (Study on Indika Energi Tbk., JII)}},
year = {2019}
}
@article{Jaakkola2006,
author = {Jaakkola, Tommi},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Mixture Models (MIT).pdf:pdf},
isbn = {0824776917},
issn = {01290657},
journal = {MIT OpenCourseware},
number = {2},
pages = {353--62},
pmid = {17117496},
title = {{Lecture 15 - Mixture Models (cont'd}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20491729},
volume = {16},
year = {2006}
}
@article{Bin2017,
author = {Bin, Riccardo De},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Advanced Regression.pdf:pdf},
pages = {1--5},
title = {{Final project STK4030 / 9030 - Statistical learning : Advanced regression and classification}},
year = {2017}
}
@article{Do2008,
abstract = {The field of epigenetics has witnessed a recent explosion in our knowledge on the importance of epigenetic events in the control of both normal cellular processes and abnormal events associated with diseases, moving this field to the forefront of biomedical research. Advances in the field of cancer epigenetics and epigenomics have turned academic, medical, and public attention to the potential application of epigenetics in cancer control. A tremendous pace of discovery in this field requires that these recent conceptual breakthroughs and technological state-of-the-art in epigenetics and epigenomics are updated and summarized in one book with cancer focus. This book is primarily intended to academic and professional audience; however, an attempt has been made to make it understandable by and appealing to a wider audience among healthcare workers. The main aim of this book is to produce an authoritative and comprehensive reference source in print and online, covering all critical aspects of epigenetics and epigenomics and their implications in cancer research. This book discusses the state of science and determines the future research needs, covering most recent advances, both conceptual and technological, and their implication for better understanding of molecular mechanisms of cancer development and progression, early detection, risk assessment, and prevention of cancer. In this chapter, we describe the main aim and scope of this book and provide a brief emphasis of each of 22 chapters regrouped into eight major parts.},
author = {Do, Chuong B},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Multivariate Gaussian Distribution.pdf:pdf},
pages = {1--10},
title = {{The Multivariate Gaussian Distribution Relationship to univariate Gaussians The covariance matrix}},
year = {2008}
}
@article{Veloso2017,
author = {Veloso, Lecturer Manuela M and Xing, Eric P},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/The EM Algorithm and Gaussian Mixtures (CMU).pdf:pdf},
pages = {1--7},
title = {{Lecture 8 : The EM algorithm K-means Algorithm Description}},
year = {2017}
}
@article{Mining2017,
author = {Mining, Data},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/ESLII_print12_toc.pdf:pdf},
title = {{Basic Stat & R}},
year = {2017}
}
@article{Ghahramani2003,
author = {Ghahramani, Zoubin},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayesian Methods for Unsupervised Learning.pdf:pdf},
journal = {Psychology},
number = {July},
pages = {48},
title = {{Bayesian Methods for Unsupervised Learning What is Unsupervised Learning ?}},
year = {2003}
}
@article{Moitra2018,
abstract = {A description of Gaussian Mixture Models as applied to instrument classification},
author = {Moitra, Ankur},
doi = {10.1017/9781316882177.008},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Gaussian Mixture Models.pdf:pdf},
journal = {Algorithmic Aspects of Machine Learning},
pages = {107--131},
title = {{Gaussian Mixture Models}},
year = {2018}
}
@article{2017,
author = {الكندي},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A Supervised Approach to Training Gaussian Mixture Models.pdf:pdf},
journal = {Occupational Medicine},
number = {4},
pages = {130},
title = {{No Titleسلطنه عمان}},
volume = {53},
year = {2017}
}
@article{Jin2016,
abstract = {We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M ≥ 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [2007]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 - e-$\Omega$(M). We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.},
archivePrefix = {arXiv},
arxivId = {1609.00978},
author = {Jin, Chi and Zhang, Yuchen and Balakrishnan, Sivaraman and Wainwright, Martin J. and Jordan, Michael I.},
eprint = {1609.00978},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Local Maxima in the Likelihood of Gaussian Mixture Models.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {4123--4131},
title = {{Local maxima in the likelihood of Gaussian Mixture Models: Structural results and algorithmic consequences}},
year = {2016}
}
@article{Moutinho2014,
abstract = {In factor analysis, the origin myth is that we have a fairly small number, q of real variables which happen to be unobserved (" latent "), and the much larger number p of variables we do observe arise as linear combinations of these factors, plus noise. The mythology is that it's possible for us (or for Someone) to continuously adjust the latent variables, and the distribution of observables likewise changes continuously. What if the latent variables are not continuous but ordinal, or even categorical? The natural idea would be that each value of the latent variable would give a different distribution of the observables. 20.1.2 From Kernel Density Estimates to Mixture Models We have also previously looked at kernel density estimation, where we approximate the true distribution by sticking a small (1 n weight) copy of a kernel pdf at each ob-served data point and adding them up. With enough data, this comes arbitrarily close to any (reasonable) probability density, but it does have some drawbacks. Sta-tistically, it labors under the curse of dimensionality. Computationally, we have to remember all of the data points, which is a lot. We saw similar problems when we looked at fully non-parametric regression, and then saw that both could be amelio-rated by using things like additive models, which impose more constraints than, say, unrestricted kernel smoothing. Can we do something like that with density estima-tion? Additive modeling for densities is not as common as it is for regression — it's harder to think of times when it would be natural and well-defined 1},
author = {Moutinho, Luiz and Hutcheson, Graeme and Sofroniou, Nick and Sofroniou, Nick},
doi = {10.4135/9781446251119.n58},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Mixture Models (CMU).pdf:pdf},
journal = {The SAGE Dictionary of Quantitative Management Research},
pages = {193--194},
title = {{Mixture Models}},
year = {2014}
}
@article{Pearson1893,
author = {Pearson, Karl},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Contributions to the mathematical theory of evolution.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {4},
pages = {675--679},
title = {{Contributions to the Mathematical Theory of Evolution}},
url = {http://www.jstor.org/stable/2979438%5Cnpapers3://publication/uuid/01543756-3D5F-4EB7-B8FF-B5E0B3C504E7},
volume = {56},
year = {1893}
}
@article{Fujiyoshi1989,
abstract = {The exact architecture of the normal nasopharyngeal tonsil remains obscure because most histopathologic investigations have been based on surgically removed adenoids. We compared enlarged adenoids and normal nasopharyngeal tonsils under both light and electron microscopes. The marked features of clinically enlarged adenoids were a large extension of the reticular epithelium and increased germinal centers. A tendency toward increased stratified squamous epithelium and decreased ciliated epithelium was apparent in enlarged adenoids, possibly due to inflammatory conditions. One type of nonciliated cell seemed to transport foreign material into underlying lymphocytes, as do the M cells of gut-associated lymphoid tissue. This type of nonciliated cell was rarely found in the extended reticular epithelium of enlarged adenoids. These findings suggest a disturbance of the antigen-trapping system and surface protections in adenoidal enlargement. {\textcopyright} 1989.},
author = {Fujiyoshi, Tatsuya and Watanabe, Tetsuo and Ichimiya, Issei and Mogi, Goro},
doi = {10.1016/0196-0709(89)90135-X},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Regularized Paths for Generalized Linear Models.pdf:pdf},
issn = {01960709},
journal = {American Journal of Otolaryngology--Head and Neck Medicine and Surgery},
keywords = {M cell,adenoid,electron microscope,nasopharyngeal tonsil},
number = {2},
pages = {124--131},
pmid = {2929880},
title = {{Functional architecture of the nasopharyngeal tonsil}},
volume = {10},
year = {1989}
}
@misc{Cheng2010,
author = {Cheng, Andria},
booktitle = {MarketWatch},
title = {{Wal-Mart says Castro-Wright leaving current post - MarketWatch}},
url = {https://www.marketwatch.com/story/wal-mart-says-castro-wright-leaving-current-post-2010-06-29},
urldate = {2021-04-14},
year = {2010}
}
@article{Wright2015,
abstract = {Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.},
archivePrefix = {arXiv},
arxivId = {1502.04759},
author = {Wright, Stephen J.},
doi = {10.1007/s10107-015-0892-3},
eprint = {1502.04759},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Coordinate Descent Algorithms.pdf:pdf},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Coordinate descent,Parallel numerical computing,Randomized algorithms},
number = {1},
pages = {3--34},
title = {{Coordinate descent algorithms}},
volume = {151},
year = {2015}
}
@article{Hastie2016,
abstract = {Statistics K25103 w w w . c r c p r e s s . c o m K25103_cover.indd 1 2/24/15 1:35 PM},
author = {Hastie, Trevor and Tibshirani, Robert and Hastie, Martin Wainwright and Tibshirani, @bullet and Wainwright, @bullet},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical Learning with Sparsity.pdf:pdf},
title = {{Statistical Learning with Sparsity Monographs on Statistics and Applied Probability 143 143 copy to come from copywriter for review}},
year = {2016}
}
@article{Luxburg2011,
archivePrefix = {arXiv},
arxivId = {0810.4752},
author = {von Luxburg, Ulrike and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1016/B978-0-444-52936-7.50016-1},
eprint = {0810.4752},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/0810.4752.pdf:pdf},
issn = {18745857},
journal = {Handbook of the History of Logic},
pages = {651--706},
title = {{Statistical Learning Theory: Models, Concepts, and Results}},
volume = {10},
year = {2011}
}
@article{Jensen1906,
author = {Jensen, J. L.W.V.},
doi = {10.1007/BF02418571},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Jensen's Inequality.pdf:pdf},
issn = {00015962},
journal = {Acta Mathematica},
number = {1},
pages = {175--193},
title = {{Sur les fonctions convexes et les in{\'{e}}galit{\'{e}}s entre les valeurs moyennes}},
volume = {30},
year = {1906}
}
@article{Gershman2012,
abstract = {A key problem in statistical modeling is model selection, that is, how to choose a model at an appropriate level of complexity. This problem appears in many settings, most prominently in choosing the number of clusters in mixture models or the number of factors in factor analysis. In this tutorial, we describe Bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. This tutorial is a high-level introduction to Bayesian nonparametric methods and contains several examples of their application. {\textcopyright} 2011 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1106.2697},
author = {Gershman, Samuel J. and Blei, David M.},
doi = {10.1016/j.jmp.2011.08.004},
eprint = {1106.2697},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/A Tutorial on Bayesian Nonparametric Models.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Bayesian methods,Chinese restaurant process,Indian buffet process},
number = {1},
pages = {1--12},
title = {{A tutorial on Bayesian nonparametric models}},
volume = {56},
year = {2012}
}
@article{Ghosh2006,
abstract = {A major issue in k-nearest neighbor classification is how to choose the optimum value of the neighborhood parameter k. Popular cross-validation techniques often fail to guide us well in selecting k mainly due to the presence of multiple minimizers of the estimated misclassification rate. This article investigates a Bayesian method in this connection, which solves the problem of multiple optimizers. The utility of the proposed method is illustrated using some benchmark data sets. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Ghosh, Anil K.},
doi = {10.1016/j.csda.2005.06.007},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/On Optimal Choice of k in Nearest Neighbor Classification.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Accuracy index,Bayesian strength function,Cross-validation,Misclassification rate,Neighborhood parameter,Non-informative prior,Optimal Bayes risk,Posterior probability},
number = {11},
pages = {3113--3123},
title = {{On optimum choice of k in nearest neighbor classification}},
volume = {50},
year = {2006}
}
@article{Chang2018,
abstract = {Mass spectrometry (MS) has become a prominent choice for large-scale absolute protein quantification, but its quantification accuracy still has substantial room for improvement. A crucial issue is the bias between the peptide MS intensity and the actual peptide abundance, i.e., the fact that peptides with equal abundance may have different MS intensities. This bias is mainly caused by the diverse physicochemical properties of peptides. Here, we propose a novel algorithm for label-free absolute protein quantification, LFAQ, which can correct the biased MS intensities by using the predicted peptide quantitative factors for all identified peptides. When validated on datasets produced by different MS instruments and data acquisition modes, LFAQ presented accuracy and precision superior to those of existing methods. In particular, it reduced the quantification error by an average of 46% for low-abundance proteins.},
author = {Chang, Cheng and Gao, Zhiqiang and Ying, Wantao and Zhao, Yan and Fu, Yan and Wu, Songfeng and Li, Mengjie and Wang, Guibin and Qian, Xiaohong and Zhu, Yunping and He, Fuchu},
doi = {10.1101/328864},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Elements of Statistical Learning.pdf:pdf},
journal = {bioRxiv},
title = {{LFAQ: Towards unbiased label-free absolute protein quantification by predicting peptide quantitative factors}},
year = {2018}
}
@article{Dempster1977a,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Maximum Likelihood Estimation of Incomplete Data and the EM Algorithm.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
number = {1},
pages = {1--22},
title = {{ Maximum Likelihood from Incomplete Data Via the EM Algorithm }},
volume = {39},
year = {1977}
}
@article{Mackey2014,
author = {Mackey, Lecturer Lester},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/stats306b-spring14-lecture3_scribed.pdf:pdf},
pages = {1--8},
title = {{Lecture 3 — April 7th Recap : Gaussian Mixture Modeling EM for General Latent Variable Models}},
volume = {1},
year = {2014}
}
@article{Zambrano2017,
author = {Zambrano, Jes{\'{u}}s},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Gaussian-Mixture-Models.pdf:pdf},
number = {November},
title = {{Gaussian Mixture Models – method and applications}},
year = {2017}
}
@article{Ghasemipour,
author = {Ghasemipour, Kamyar},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Clustering K-Means EM Tutorial.pdf:pdf},
title = {{Clustering, K-Means, EM Tutorial}},
url = {http://www.cs.toronto.edu/$\sim$jlucas/teaching/csc411/lectures/tut8_handout.pdf}
}
@article{Evgeniou2000,
abstract = {In this paper we first overview the main concepts of Statistical Learning Theory, a framework in which learning from examples can be studied in a principled way. We then briefly discuss well known as well as emerging learning techniques such as Regularization Networks and Support Vector Machines which can be justified in term of the same induction principle. {\textcopyright} 2000 Kluwer Academic Publishers.},
author = {Evgeniou, Theodoros and Pontil, Massimiliano and Poggio, Tomaso},
doi = {10.1023/A:1008110632619},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical Learning Theory - A Primer.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {1},
pages = {9--13},
title = {{Statistical Learning Theory: A primer}},
volume = {38},
year = {2000}
}
@article{Signal2017,
author = {Signal, Statistical and Lecture, Processing and Scribe, Jiantao Jiao and Hilger, Andrew and Wald, Abraham},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical Decision Theory Lecture Notes (Stanford).pdf:pdf},
number = {1},
pages = {342--357},
title = {{Lecture 2 : Statistical Decision Theory Basic Elements of Statistical Decision Theory Optimality Criterion of Decision Rules}},
year = {2017}
}
@article{McLachlan2019,
abstract = {The important role of finite mixture models in the statistical analysis of data is underscored by the ever-increasing rate at which articles on mixture applications appear in the statistical and general scientific literature. The aim of this article is to provide an up-to-date account of the theory and methodological developments underlying the applications of finite mixture models. Because of their flexibility, mixture models are being increasingly exploited as a convenient, semiparametric way in which to model unknown distributional shapes. This is in addition to their obvious applications where there is group-structure in the data or where the aim is to explore the data for such structure, as in a cluster analysis. It has now been three decades since the publication of the monograph by McLachlan & Basford (1988) with an emphasis on the potential usefulness of mixture models for inference and clustering. Since then, mixture models have attracted the interest of many researchers and have found many new and interesting fields of application. Thus, the literature on mixture models has expanded enormously, and as a consequence, the bibliography here can only provide selected coverage.},
author = {McLachlan, Geoffrey J. and Lee, Sharon X. and Rathnayake, Suren I.},
doi = {10.1146/annurev-statistics-031017-100325},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Finite Mixture Models (McLachlan 2019).pdf:pdf},
issn = {2326831X},
journal = {Annual Review of Statistics and Its Application},
keywords = {EM algorithm,mixture proportions,mixtures of factor analyzers,model-based clustering,normal and t -mixture distributions},
number = {1988},
pages = {355--378},
title = {{Finite mixture models}},
volume = {6},
year = {2019}
}
@article{Haas2002,
author = {Haas, Shane M},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/summary.pdf:pdf},
journal = {Presentation},
number = {1},
pages = {1--21},
title = {{The Expectation-Maximization and Alternating Minimization Algorithms}},
volume = {1},
year = {2002}
}
@article{Wu2008,
abstract = {Imposition of a lasso penalty shrinks parameter estimates toward zero and performs continuous model selection. Lasso penalized regression is capable of handling linear regression problems where the number of predictors far exceeds the number of cases. This paper tests two exceptionally fast algorithms for estimating regression coefficients with a lasso penalty. The previously known ℓ 2 algorithm is based on cyclic coordinate descent. Our new ℓ 1 algorithm is based on greedy coordinate descent and Edgeworth's algorithm for ordinary ℓ 1 regression. Each algorithm relies on a tuning constant that can be chosen by cross-validation. In some regression problems it is natural to group parameters and penalize parameters group by group rather than separately. If the group penalty is proportional to the Euclidean norm of the parameters of the group, then it is possible to majorize the norm and reduce parameter estimation to ℓ 2 regression with a lasso penalty. Thus, the existing algorithm can be extended to novel settings. Each of the algorithms discussed is tested via either simulated or real data or both. The Appendix proves that a greedy form of the ℓ 2 algorithm converges to the minimum value of the objective function. {\textcopyright} Institute of Mathematical Statistics.},
archivePrefix = {arXiv},
arxivId = {arXiv:0803.3876v1},
author = {Wu, Tong Tong and Lange, Kenneth},
doi = {10.1214/07-AOAS147},
eprint = {arXiv:0803.3876v1},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Cooridinate Descent Algorithms for Lasso.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Consistency,Convergence,Cyclic,Edgeworth's algorithm,Greedy,Model selection},
number = {1},
pages = {224--244},
title = {{Coordinate descent algorithms for lasso penalized regression}},
volume = {2},
year = {2008}
}
@article{Classical2015,
abstract = {We begin by assuming that the complete data-set consists of Z = (X , Y) but that only X is observed. The complete-data log likelihood is then denoted by l($\theta$; X , Y) where $\theta$ is the unknown parameter vector for which we wish to find the MLE. E-Step: The E-step of the EM algorithm computes the expected value of l($\theta$; X , Y) given the observed data, X , and the current parameter estimate, $\theta$ old say. In particular, we define Q($\theta$; $\theta$ old) := E [l($\theta$; X , Y) | X , $\theta$ old ] = l($\theta$; X , y) p(y | X , $\theta$ old) dy (1) where p({\textperiodcentered} | X , $\theta$ old) is the conditional density of Y given the observed data, X , and assuming $\theta$ = $\theta$ old. M-Step: The M-step consists of maximizing over $\theta$ the expectation computed in (1). That is, we set $\theta$ new := max $\theta$ Q($\theta$; $\theta$ old). We then set $\theta$ old = $\theta$ new. The two steps are repeated as necessary until the sequence of $\theta$ new 's converges. Indeed under very general circumstances convergence to a local maximum can be guaranteed and we explain why this is the case below. If it is suspected that the log-likelihood function has multiple local maximums then the EM algorithm should be run many times, using a different starting value of $\theta$ old on each occasion. The ML estimate of $\theta$ is then taken to be the best of the set of local maximums obtained from the various runs of the EM algorithm. Why Does the EM Algorithm Work? We use p({\textperiodcentered} | {\textperiodcentered}) to denote a generic conditional PDF. Now observe that l($\theta$; X) = ln p(X | $\theta$) = ln p(X , y | $\theta$) dy = ln p(X , y | $\theta$) p(y | X , $\theta$ old) p(y | X , $\theta$ old) dy},
author = {Classical, The and Algorithm, E M},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/EM_Algorithm.pdf:pdf},
title = {{IEOR E4570: Machine Learning for OR&FE The EM Algorithm}},
year = {2015}
}
@article{Course2003,
author = {Course, Asci and Issues, Advanced},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/mixtures-em.pdf:pdf},
number = {March},
pages = {1--3},
title = {{Gaussian mixtures and the EM algorithm}},
year = {2003}
}
@article{sims1980,
abstract = {Existing strategies for econometric analysis related to macroeconomics are subject to a number of serious objections, some recently formulated, some old. These objections are summarized in this paper, and it is argued that taken together they make it unlikely that macroeconomic models are in fact over identified, as the existing statistical theory usually assumes. The implications of this conclusion are explored, and an example of econometric work in a non-standard style, taking account of the objections to the standard style, is presented.},
author = {Sims, Christopher A},
issn = {00129682, 14680262},
journal = {Econometrica},
number = {1},
pages = {1--48},
publisher = {[Wiley, Econometric Society]},
title = {{Macroeconomics and Reality}},
url = {http://www.jstor.org/stable/1912017},
volume = {48},
year = {1980}
}
@article{Morris,
archivePrefix = {arXiv},
arxivId = {arXiv:1308.6315v3},
author = {Morris, Katherine and Mcnicholas, Paul D},
eprint = {arXiv:1308.6315v3},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Clustering, Classification, Discriminant Analysis.pdf:pdf},
keywords = {dimension reduction,generalized hyperbolic distribution,mixture models,model-based classification,model-based clustering,model-based discriminant analysis},
number = {Section 3},
title = {{Reduction via Generalized Hyperbolic Mixtures arXiv : 1308 . 6315v3 [ stat . ME ] 23 Jun 2015}}
}
@misc{Gold2011,
abstract = { the ease with which we recognize a face, understand spoken words, read handwrit-ten characters, identify our car keys in our pocket by feel, and decide whether an apple is ripe by its smell belies the astoundingly complex processes that underlie these acts ofpattern recognition.   Pattern recognition — the act oftaking in raw data and taking an action based on the “category” ofthe pattern — has been crucial for our survival, and over the past tens of millions of years we have evolved highly sophesticated neutral and cognitive systems for such tasks.},
author = {Gold, Ben and Morgan, Nelson and Ellis, Dan},
booktitle = {Speech and Audio Signal Processing},
doi = {10.1002/9781118142882.ch8},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Pattern Classification (Duda, Stork, Hart).pdf:pdf},
pages = {105--123},
title = {{Pattern Classification}},
year = {2011}
}
@article{Hastie2009,
author = {Hastie, Trevor},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Fast Regularization Paths via Coordinate Descent.pdf:pdf},
journal = {Statistics},
pages = {1--33},
title = {{Fast Regularization Paths via Coordinate Descent}},
year = {2009}
}
@article{Pentakalos2019,
abstract = {Talked about what Machine Learning is - Reviewed some of the key problems machine learning solves - Reviewed some of the key algorithms - Looked at an example - Any Questions?.},
author = {Pentakalos, Odysseas},
doi = {10.4018/978-1-7998-0414-7.ch003},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/18-EM.pdf:pdf},
issn = {09521976},
journal = {Cmg Impact 2019},
title = {{Introduction to machine learning}},
year = {2019}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Maximum Likelihood Estimation and the EM Algorithm.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
number = {1},
pages = {1--22},
title = {{ Maximum Likelihood from Incomplete Data Via the EM Algorithm }},
volume = {39},
year = {1977}
}
@article{Letham2012,
abstract = {Purpose/Objective(s): To determine factors predictive of radiation retinopathy (RR) and visual loss (20/200 or worse) following I-125 and Ru-106 plaque brachytherapy for uveal melanoma. Materials/Methods: All patients treated at the Cleveland Clinic for a primary single unilateral ciliary body or choroid melanoma between 1/1/2005 and 6/30/2010 with I-125 or Ru-106 plaque brachytherapy were included. Patients treated for iris melanomas were excluded from analysis. Patients with bilateral melanomas or who had been previously treated with other methods, including transpupillary thermotherapy, were also excluded. Primary endpoints were loss of visual acuity and development of RR. Factors analyzed were gender, age, history of cataracts, history of diabetes, tumor size (basal dimension and apical height), tumor location, and radiation dose to the apex, fovea, and optic disc. Multivariate and univariate Cox proportional hazards were used to determine the influence of baseline patient factors on the development of RR or vision loss. Kaplan-Meier curves (log rank analysis) were used to estimate freedom from RR and vision loss. Results: One hundred eighty-nine patients were eligible, of which 174 were still alive as of February 1, 2011. Of the 15 deaths, 4 were related to uveal melanoma, 5 were unrelated, and 6 were due to unknown causes. 6 patients required enucleation for continued tumor growth and 1 required enucleation due to painful glaucoma. 12 patients developed metastases to the liver, with 1 also developing lung metastases. 53 developed RR, 116 had post-treatment visual acuity loss ≤ 20/50 and 58 had post-treatment visual acuity loss ≤ 20/200. The only post treatment related side effects were RR or loss of visual acuity. No patients required additional laser treatments, developed ocular infections, or required re-admissions within 30 days of the procedure. On multivariate analysis, increased age (HR of 0.97[0.95-0.99], p = 0.003) and distance to the optic disc (HR of 0.91 [0.85-0.97], p = 0.006) were protective against the development of RR, while an increase in the apical height of the tumor (HR of 1.18 [1.06-1.31], p = 0.003) was detrimental. Increases in age (HR of 1.01[1.00-1.03], p = 0.05), increases in apical height (HR of 1.35 [1.22-1.48], p<0.001), and a greater total dose to the fovea (HR of 1.01 [1.01-1.01], p<0.001) were predictive of mild or severe vision loss. Conclusions: Although plaque radiotherapy offers excellent tumor control (97%), almost one third of patients are left with visual acuity of≤20/200. Modifiable factors associated with poorer visual outcomes such as total doses to the fovea and optic disc could be reduced by the use of alternative isotopes and improvements in plaque design.},
author = {Letham, Ben and Rudin, Cynthia},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Bayesian Analysis - MIT.pdf:pdf},
journal = {Prediction: Machine Learning and Statistics Lecture Notes},
pages = {1--42},
title = {{Probabilistic Modeling and Bayesian Analysis}},
url = {http://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/},
year = {2012}
}
@article{Permuter,
author = {Permuter, Lecturer Haim},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/GMM and EM.pdf:pdf},
pages = {1--16},
title = {{Lecture 2 : GMM and EM}}
}
@article{Baker2005,
abstract = {Most tutorials on complex topics are apparently written by very smart people whose goal is to use as little space as possible and who assume that their readers already know almost as much as the author does. This tutorial's not like that. It's more a manifestivus for the rest of us. It's about the mechanics of singular value decomposition, especially as it relates to some techniques in natural language processing. It's written by someone who knew zilch about singular value decomposition or any of the underlying math before he started writing it, and knows barely more than that now. Accordingly, it's a bit long on the background part, and a bit short on the truly explanatory part, but hopefully it contains all the information necessary for someone who's never heard of singular value decomposition before to be able to do it.},
author = {Baker, Kirk},
doi = {10.1021/jo0008901},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Singular_Value_Decomposition_Tutorial.pdf:pdf},
isbn = {8479388927},
issn = {00223263},
journal = {The Ohio State University},
pages = {24},
pmid = {11429886},
title = {{Singular value decomposition tutorial}},
url = {http://www.ling.ohio-state.edu/$\sim$kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf%5Cnhttp://www.ams.org/samplings/feature-column/fcarc-svd},
volume = {2005},
year = {2005}
}
@article{VonLuxburg2009,
abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
archivePrefix = {arXiv},
arxivId = {0810.4752},
author = {von Luxburg, Ulrike and Schoelkopf, Bernhard},
eprint = {0810.4752},
file = {:C\:/Users/John/Documents/Knowledge Centre/Data Science/Statistical Learning/Statistical-Decision Theory (PSU).pdf:pdf},
journal = {Handbook of the History of Logic.},
pages = {651--706},
title = {{Statistical Learning Theory: Models, Concepts, and Results}},
url = {http://arxiv.org/abs/0810.4752},
volume = {10},
year = {2009}
}
