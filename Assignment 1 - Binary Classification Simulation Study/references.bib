
@book{hastie01statisticallearning,
	address = {New York, NY, USA},
	title = {The {Elements} of {Statistical} {Learning}},
	abstract = {The area's standard text revised and expanded. During the past decade has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization and spectral clustering. There is also a chapter on methods for ``wide'' data ( p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.},
	publisher = {Springer New York Inc.},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2001},
	note = {Series Title: Springer Series in Statistics},
	keywords = {psl},
	file = {The Elements of Statistical Learning-metadata.md:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\references\\The Elements of Statistical Learning-metadata.md:text/plain},
}

@article{JamesHastie2013,
	title = {An {Introduction} to {Statistical} {Learning}},
	volume = {8},
	issn = {01621459},
	url = {https://www-bcf.usc.edu/%7B~%7Dgareth/ISL/},
	doi = {10.1016/j.peva.2007.06.006},
	abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\textasciitilde}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
	author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Tibshirani, Robert},
	year = {2013},
	pmid = {10911016},
	note = {arXiv: 1011.1669v3
ISBN: 9780387781884},
	keywords = {psl},
}

@article{Zhang2017b,
	title = {An up-to-date comparison of state-of-the-art classification algorithms},
	volume = {82},
	issn = {09574174},
	url = {http://dx.doi.org/10.1016/j.eswa.2017.04.003},
	doi = {10.1016/j.eswa.2017.04.003},
	abstract = {Current benchmark reports of classification algorithms generally concern common classifiers and their variants but do not include many algorithms that have been introduced in recent years. Moreover, important properties such as the dependency on number of classes and features and CPU running time are typically not examined. In this paper, we carry out a comparative empirical study on both established classifiers and more recently proposed ones on 71 data sets originating from different domains, publicly available at UCI and KEEL repositories. The list of 11 algorithms studied includes Extreme Learning Machine (ELM), Sparse Representation based Classification (SRC), and Deep Learning (DL), which have not been thoroughly investigated in existing comparative studies. It is found that Stochastic Gradient Boosting Trees (GBDT) matches or exceeds the prediction performance of Support Vector Machines (SVM) and Random Forests (RF), while being the fastest algorithm in terms of prediction efficiency. ELM also yields good accuracy results, ranking in the top-5, alongside GBDT, RF, SVM, and C4.5 but this performance varies widely across all data sets. Unsurprisingly, top accuracy performers have average or slow training time efficiency. DL is the worst performer in terms of accuracy but second fastest in prediction efficiency. SRC shows good accuracy performance but it is the slowest classifier in both training and testing.},
	journal = {Expert Systems with Applications},
	author = {Zhang, Chongsheng and Liu, Changchang and Zhang, Xiangliang and Almpanidis, George},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Classification benchmarking, Classifier comparison, Classifier evaluation, psl},
	pages = {128--150},
	file = {PDF:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\32S6EZI6\\An up-to-date comparison of state-of-the-art classification algorithms.pdf:application/pdf},
}

@article{guang-binhuangExtremeLearningMachine2012,
	title = {Extreme {Learning} {Machine} for {Regression} and {Multiclass} {Classification}},
	volume = {42},
	issn = {1083-4419, 1941-0492},
	url = {http://ieeexplore.ieee.org/document/6035797/},
	doi = {10.1109/TSMCB.2011.2168604},
	number = {2},
	urldate = {2021-02-12},
	journal = {IEEE Trans. Syst., Man, Cybern. B},
	author = {{Guang-Bin Huang} and {Hongming Zhou} and {Xiaojian Ding} and {Rui Zhang}},
	month = apr,
	year = {2012},
	keywords = {psl},
	pages = {513--529},
}

@article{friedmanStochasticGradientBoosting2002,
	title = {Stochastic gradient boosting},
	volume = {38},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947301000652},
	doi = {10.1016/S0167-9473(01)00065-2},
	language = {en},
	number = {4},
	urldate = {2021-02-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Friedman, Jerome H.},
	month = feb,
	year = {2002},
	keywords = {psl},
	pages = {367--378},
}

@article{cortesSupportvectorNetworks1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	language = {en},
	number = {3},
	urldate = {2021-02-12},
	journal = {Mach Learn},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	keywords = {psl},
	pages = {273--297},
	file = {Support-vector networks.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Support-vector networks.pdf:application/pdf},
}

@article{tinkamhoRandomSubspaceMethod1998,
	title = {The random subspace method for constructing decision forests},
	volume = {20},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/709601/},
	doi = {10.1109/34.709601},
	number = {8},
	urldate = {2021-02-12},
	journal = {IEEE Trans. Pattern Anal. Machine Intell.},
	author = {{Tin Kam Ho}},
	month = aug,
	year = {1998},
	keywords = {psl},
	pages = {832--844},
	file = {The random subspace method for constructing decision forests.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\The random subspace method for constructing decision forests.pdf:application/pdf},
}

@article{wuTop10Algorithms2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-007-0114-2},
	doi = {10.1007/s10115-007-0114-2},
	language = {en},
	number = {1},
	urldate = {2021-02-12},
	journal = {Knowl Inf Syst},
	author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
	month = jan,
	year = {2008},
	keywords = {psl},
	pages = {1--37},
	file = {Top 10 algorithms in data mining.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Top 10 algorithms in data mining.pdf:application/pdf},
}

@article{fixDiscriminatoryAnalysisNonparametric1989,
	title = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}: {Consistency} {Properties}},
	volume = {57},
	issn = {03067734},
	shorttitle = {Discriminatory {Analysis}. {Nonparametric} {Discrimination}},
	url = {https://www.jstor.org/stable/1403797?origin=crossref},
	doi = {10.2307/1403797},
	number = {3},
	urldate = {2021-02-12},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Fix, Evelyn and Hodges, J. L.},
	month = dec,
	year = {1989},
	keywords = {psl},
	pages = {238},
}

@article{LIIEssaySolving1763,
	title = {{LII}. {An} essay towards solving a problem in the doctrine of chances. {By} the late {Rev}. {Mr}. {Bayes}, {F}. {R}. {S}. communicated by {Mr}. {Price}, in a letter to {John} {Canton}, {A}. {M}. {F}. {R}. {S}},
	volume = {53},
	issn = {0261-0523, 2053-9223},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstl.1763.0053},
	doi = {10.1098/rstl.1763.0053},
	abstract = {Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
	language = {la},
	urldate = {2021-02-12},
	journal = {Phil. Trans. R. Soc.},
	month = dec,
	year = {1763},
	keywords = {psl},
	pages = {370--418},
	file = {LII.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\LII.pdf:application/pdf},
}

@article{berksonApplicationLogisticFunction1944,
	title = {Application to the {Logistic} {Function} to {Bio}-{Assay}},
	volume = {39},
	issn = {01621459},
	url = {https://www.jstor.org/stable/2280041?origin=crossref},
	doi = {10.2307/2280041},
	number = {227},
	urldate = {2021-02-12},
	journal = {Journal of the American Statistical Association},
	author = {Berkson, Joseph},
	month = sep,
	year = {1944},
	keywords = {psl},
	pages = {357},
}

@article{vapnikOverviewStatisticalLearning1999,
	title = {An overview of statistical learning theory},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/788640/},
	doi = {10.1109/72.788640},
	number = {5},
	urldate = {2021-02-12},
	journal = {IEEE Trans. Neural Netw.},
	author = {Vapnik, V.N.},
	month = sep,
	year = {1999},
	keywords = {psl},
	pages = {988--999},
	file = {An overview of statistical learning theory.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\An overview of statistical learning theory.pdf:application/pdf},
}

@article{vonluxburgStatisticalLearningTheory2008,
	title = {Statistical {Learning} {Theory}: {Models}, {Concepts}, and {Results}},
	shorttitle = {Statistical {Learning} {Theory}},
	url = {http://arxiv.org/abs/0810.4752},
	abstract = {Statistical learning theory provides the theoretical basis for many of today's machine learning algorithms. In this article we attempt to give a gentle, non-technical overview over the key ideas and insights of statistical learning theory. We target at a broad audience, not necessarily machine learning researchers. This paper can serve as a starting point for people who want to get an overview on the field before diving into technical details.},
	urldate = {2021-02-12},
	journal = {arXiv:0810.4752 [math, stat]},
	author = {von Luxburg, Ulrike and Schoelkopf, Bernhard},
	month = oct,
	year = {2008},
	note = {arXiv: 0810.4752},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, psl},
	file = {Statistical Learning Theory.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Statistical Learning Theory.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\John\\Documents\\Knowledge Centre\\Zotero\\storage\\3EUI3Y5P\\0810.html:text/html},
}

@article{bengioLearningDeepArchitectures2009,
	title = {Learning {Deep} {Architectures} for {AI}},
	volume = {2},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-006},
	doi = {10.1561/2200000006},
	language = {en},
	number = {1},
	urldate = {2021-02-13},
	journal = {FNT in Machine Learning},
	author = {Bengio, Y.},
	year = {2009},
	keywords = {psl},
	pages = {1--127},
	file = {Learning Deep Architectures for AI.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\Learning Deep Architectures for AI.pdf:application/pdf},
}

@article{doganComparativeAnalysisClassification2013,
	title = {A comparative analysis of classification algorithms in data mining for accuracy, speed and robustness},
	volume = {14},
	issn = {1385-951X, 1573-7667},
	url = {http://link.springer.com/10.1007/s10799-012-0135-8},
	doi = {10.1007/s10799-012-0135-8},
	language = {en},
	number = {2},
	urldate = {2021-02-13},
	journal = {Inf Technol Manag},
	author = {Dogan, Neslihan and Tanrikulu, Zuhal},
	month = jun,
	year = {2013},
	keywords = {psl},
	pages = {105--124},
}

@article{wolpertNoFreeLunch1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/585893/},
	doi = {10.1109/4235.585893},
	number = {1},
	urldate = {2021-02-13},
	journal = {IEEE Trans. Evol. Computat.},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	keywords = {psl},
	pages = {67--82},
	file = {No free lunch theorems for optimization.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\No free lunch theorems for optimization.pdf:application/pdf},
}

@article{khondokerComparisonMachineLearning2016,
	title = {A comparison of machine learning methods for classification using simulation with multiple real data examples from mental health studies},
	volume = {25},
	issn = {0962-2802, 1477-0334},
	url = {http://journals.sagepub.com/doi/10.1177/0962280213502437},
	doi = {10.1177/0962280213502437},
	abstract = {Background
              Recent literature on the comparison of machine learning methods has raised questions about the neutrality, unbiasedness and utility of many comparative studies. Reporting of results on favourable datasets and sampling error in the estimated performance measures based on single samples are thought to be the major sources of bias in such comparisons. Better performance in one or a few instances does not necessarily imply so on an average or on a population level and simulation studies may be a better alternative for objectively comparing the performances of machine learning algorithms.
            
            
              Methods
              We compare the classification performance of a number of important and widely used machine learning algorithms, namely the Random Forests (RF), Support Vector Machines (SVM), Linear Discriminant Analysis (LDA) and k-Nearest Neighbour (kNN). Using massively parallel processing on high-performance supercomputers, we compare the generalisation errors at various combinations of levels of several factors: number of features, training sample size, biological variation, experimental variation, effect size, replication and correlation between features.
            
            
              Results
              For smaller number of correlated features, number of features not exceeding approximately half the sample size, LDA was found to be the method of choice in terms of average generalisation errors as well as stability (precision) of error estimates. SVM (with RBF kernel) outperforms LDA as well as RF and kNN by a clear margin as the feature set gets larger provided the sample size is not too small (at least 20). The performance of kNN also improves as the number of features grows and outplays that of LDA and RF unless the data variability is too high and/or effect sizes are too small. RF was found to outperform only kNN in some instances where the data are more variable and have smaller effect sizes, in which cases it also provide more stable error estimates than kNN and LDA. Applications to a number of real datasets supported the findings from the simulation study.},
	language = {en},
	number = {5},
	urldate = {2021-02-13},
	journal = {Stat Methods Med Res},
	author = {Khondoker, Mizanur and Dobson, Richard and Skirrow, Caroline and Simmons, Andrew and Stahl, Daniel},
	month = oct,
	year = {2016},
	keywords = {psl},
	pages = {1804--1823},
	file = {A comparison of machine learning methods for classification using simulation.pdf:C\:\\Users\\John\\Documents\\Obsidian\\nov8ai\\A comparison of machine learning methods for classification using simulation.pdf:application/pdf},
}
