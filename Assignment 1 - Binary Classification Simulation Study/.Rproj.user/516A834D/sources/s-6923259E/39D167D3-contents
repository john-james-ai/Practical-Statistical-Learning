```{r unset_bigmark, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark="")
})
```
# Introduction
Extreme Learning Machine (ELM) [@guang-binhuangExtremeLearningMachine2012], Stochastic Gradient Boosting Decision Trees [@friedmanStochasticGradientBoosting2002], Random Forests [@tinkamhoRandomSubspaceMethod1998], and Support Vector Machines [@cortesSupportvectorNetworks1995] are among the recent years' state-of-the-art, highly accurate  classification algorithms. Yet relatively simple algorithms such as Linear Regression, Logistic Regression [@berksonApplicationLogisticFunction1944], Naive Bayes' [@LIIEssaySolving1763] and k-Nearest Neighbors (KNN) [@fixDiscriminatoryAnalysisNonparametric1989] are considered to be among the most prevalent techniques in Artificial Intelligence (AI) [@wuTop10Algorithms2008].

This paper records a simulation study of the performance and characteristics of four (plus one) of the most widely-used and understood algorithms in machine learning, namely:

- Linear Regression
- Quadratic Regression
- Naive Bayes'
- k-Nearest Neighbors
- Logistic Regression (baseline)

Using a small, yet diverse collection of data sets, some simulated, we set out to analyze algorithm quality (accuracy) and efficiency (running time and memory usage) on the binary classification tasks. Concretely, the goal was to address the following questions.

1. Which algorithm(s) perform best and on which data sets?
2. Which algorithms are the most efficient in terms of run time and memory usage?
3. What are the characteristics of the binary classification problems for which linear regression performs well?
4. How do these algorithms compare to Logistic Regression, our baseline algorithm for binary classification.

In this paper, the general performance and characteristics of the algorithms are inferred on the basis results obtained using training and test data. Statistical learning theory provides the justification and the principled framework that allows us to draw such inferences. Let's briefly visit the theoretical justification for this approach.

## Statistical Framework for Evaluation
Statistical Learning Theory (SLT), a theoretical approach to machine learning, attempts to lay the mathematical foundation for some of the most fundamental questions in the machine learning field, among them [@vonluxburgStatisticalLearningTheory2008]:        
- Which learning tasks can be performed by computers in general?
- What are the key properties a learning algorithm needs to satisfy in order to be successful?    
- Which performance guarantees can we give on the results of certain learning algorithms?       

Fortunately, the theory for supervised learning, generally and binary classification, specifically, is quite mature and frames the problem of algorithm evaluation well. 

### The Setup
For supervised learning of binary classification, we have:    
  1. **Probability Distribution** comprised of a vector space of all possible inputs, $X$, and a vector space of all possible outputs $y$.
  2 **
  **input space** $X$,      
- an **output space** $y$, which we will label with the set {0,1}

The 





## The Algorithms
### Linear Regression
Linear regression is a probabilistic model for linear data that assumes that each target value, $y_i$ is a function of one or more features $x_i$ and is generated by some unknown true function:

$$y = \theta_0+\displaystyle\sum_{i=1}^p\theta_ix_i$$.
Here our estimate of $y$, denoted by $\hat{y}$ is given by:

$$\hat{y} = \hat{\theta}_0+\displaystyle\sum_{i=1}^p\hat{\theta}_ix_i+\epsilon_i$$.
By convention and convenience, a 1 is included as a constant variable $x_0$ in the matrix $X$ formed by the vectors $x_i$, which allows us to express the linear model is a more compact form as follows:
$$\hat{y}=X^T \hat{\theta}+\epsilon$$.

The fact that our model doesn't fit the data perfectly is captured in the vector quantity $e$, noise that we assume is Gaussian: $\epsilon \sim \mathcal{N}(0,\sigma^2)$. We also assume that the true values of the parameters $\theta$, the error $\epsilon$, and its variance, $\sigma^2$, are all fixed (deterministic), but unknown. 

#### Estimating Model Parameters via Least Squares

Whether the parameters of this linear model can be solved analytically using calculus and a bit linear algebra, or numerically with calculus and a bit of linear algebra, depends upon properties of the data. If $X$ is full rank i.e. the columns are uncorrelated, the model parameters $\hat{\theta}$ may be estimated analytically by setting the derivative of the model equation to zero and solving for $\theta$. If, on the other hand, multicollinearity is extant, a numerical estimation method such as gradient descent must be used. In either case, parameter estimation is related to minimizing the **least squares** cost function.
$$J(\theta)=\frac{1}{2n}\displaystyle\sum_{i=1}^n(\theta x^{(i)}-y^{(i)})^2$$
where the superscript $^{(i)}$ denotes the observation number. To put this in matrix format, we have:
$$J(\theta)=\frac{1}{2n}(X\theta - y)^T(X\theta-y)$$
A bit of algebraic simplification yields:

$$J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty$$
Taking the derivative of both sides and setting it to zero, gives:

$$\frac{\partial J}{\partial \theta}=2X^TX\theta-2X^{T}y=0.$$
With a bit more manipulation, we have:
$$X^TX\theta=X^{T}y$$

If $X^TX$ is invertible, i.e. $X$ is full rank we can multiply both sides by $(X^TX)^{-1}$ to obtain the **normal equation**.
$$\theta=(X^TX)^{-1}X^Ty$$


