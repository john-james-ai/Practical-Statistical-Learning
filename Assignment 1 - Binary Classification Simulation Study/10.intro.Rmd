```{r unset_bigmark, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark="")
})
```
# Introduction
The research community has proposed a host of classification algorithms in recent years, some of which, i.e. Deep Learning [@bengioLearningDeepArchitectures2009], Extreme Learning Machines [@guang-binhuangExtremeLearningMachine2012], and Stochastic Gradient Boosting Decision Trees [@friedmanStochasticGradientBoosting2002] have achieved state-of-the-art performance and are widely acknowledged to be highly accurate.

Yet, relatively simple, but foundational algorithms such as Linear Regression, Logistic Regression [@berksonApplicationLogisticFunction1944], Naive Bayes' [@LIIEssaySolving1763] and k-Nearest Neighbors (KNN) [@fixDiscriminatoryAnalysisNonparametric1989] remain among the most prevalent classifiers in industry [@doganComparativeAnalysisClassification2013], [@wuTop10Algorithms2008]. 

Still, selecting the right algorithm (and lunch, apparently [@wolpertNoFreeLunch1997]) for a specific classification problem is complicated, data-dependent and primarily ad-hoc. Though no dearth of comparative studies on classification algorithms is extant, many studies compare classification performance estimated on real data sets [@Zhang2017b]. Since performance estimates are subject to sampling variability, trials conducted using real data sets may produce performance results that do not generalize to applications in which the properties of the data vary significantly from those of the trial [@khondokerComparisonMachineLearning2016]. 

In this study, we undertake an extensive simulation experiment to compare the performance and characteristics of four (plus one) of the most widely-used classifier algorithms in machine learning, namely:

* Linear Regression
* Quadratic Regression
* Naive Bayes'
* k-Nearest Neighbors
* Logistic Regression (baseline)

We compare performance measures, computation times, and memory resources of the algorithms on simulated data with varying data generating distributions, numbers of features, training sample sizes, degrees of replication and feature correlations and interactions to understand:

* Which algorithm(s) are the most accurate and on which data sets?      
* Which algorithms are the most efficient in terms of run time and memory usage?          
* How do regression and quadratic regression techniques perform in binary classification settings?   
* Which algorithms respond positively or negatively to which data set characteristics?  

Repeatedly estimating performance on a large and varied number of simulated data sets, then averaging out the sampling variability provides a robust, statistically sound, and analytical method for objectively estimating generalized performance of binary classifiers across a series of diverse settings.





Generalized performance and behavior inferences are based upon observed outcomes on available training and test data sets. Our ability to generalize such observations is justified by Statistical Learning Theory (STL), the theoretical branch of machine learning that attempts to lay the mathematical foundations for the field.

## Statistical Learning Theory
To motivate our discussion, consider the task of evaluating several algorithms that predict whether or not a patient has amyotrophic lateral sclerosis based upon imaging, demographic, laboratory, clinical, medication, and family history data obtained from the patient's medical record. The statistical learning framework has six main components:

  1. **Data Generating Distribution $P$**: This is the unknown population data distribution $P$ of features $\mathcal{X}$ and labels $\mathcal{Y}$, assumed to have a **fixed**, but unknown conditional probability $P(Y|X)$. Using our contrivance, $P$ would be the population of all medical records for any person who might or might not have or will ever have amyotrophic lateral sclerosis.

  2. **Sample $S$**: In order to evaluate our algorithms, we need access to a sample $S$, also called a **training set** $(X_1, Y_1), ..., (X_n,Y_n) \in \mathcal{X}\times \mathcal{Y}$, which is independent and identically distributed from the data generating distribution $P$. This would be the sample of medical records available for training the algorithms.

  3. **Hypothesis $h$**: The learning objective is to approximate the Oracle, a fixed, but unknown function that returns an output vector of predictions $y$ for every input vector of medical data $x$, according to the conditional distribution function $P(Y|X)$. In classification settings, this classifier is often referred to as the **Bayes Classifier**.

  4. **Hypothesis Space $H$**:  The set of all possible, yet feasible candidate hypothesis functions $f: \mathcal{X}\to \mathcal{Y}$ under consideration. Here, we consider the combinatorial group of algorithms along with all possible values of hyperparameters. 

  5. **Learning Algorithm $A$**: The learning algorithm is capable of implementing and evaluating our set of hypothesis functions $f(x,h), h\in H$ 

  6. **Loss Function $l(h_w,(x^{(i)},y^{(i)}))$**: Finally, we have a loss function $l$ that quantifies a "price" for classifying instance $x \in \mathcal{X}$ as $y \n \mathcal{Y}$. We typically use the term "loss" when referring to a single data point. For a set of sample observations, we use the term, "cost", to quantify how well the model produced by the algorithm fits the data.

### Empirical Risk Minimization
Whereas the loss function tells us how well our model performs on a single data point, a **risk** of a function is the average loss computed over data points generated according to the following underlying, yet unknown distribution $P$ [@vonluxburgStatisticalLearningTheory2008],

$$R(f):=E(l(X,Y,f(X))).$$
That is the risk of classifier $f$ is the expected loss, i.e. misclassifications, of the function $f$ overall all points $X\in \mathcal{X}$. The best classifier is the one that has the smallest possible $R(f)$. This oracle, also known as the Bayes Classifier has the following property:

$$
\begin{equation}
  f_{Bayes}(x):=
  \begin{cases}
    1 & \text{if P(Y=1|X=x)} \ge 0.5\\
    0 & \text{otherwise.}
  \end{cases}
  \end{equation}
$$
In other words, the so-called "Bayes" classifier examines the *True* probability $P(Y=1|X=x)$ and makes a prediction based upon the certainty that $Y=1$. This best classifier is defined as:

$$f_{\mathcal{F}}=\text{argmin }R(f).$$

Of course, we can never compute the risk of the Bayes classifier because the underlying probability distribution is unknown. But we can now articulate the standard problem of binary classification as follows [@vonluxburgStatisticalLearningTheory2008]:

>Binary Classification: Given some training points $(X_1, Y_1),...,(X_n,Y_n)$, which have been drawn independently from some unknown probability distribution $P$, and given some loss function $l$, how can we construct a function $f:\mathcal{X}\to\mathcal{Y}$ which has risk  $R(f)$ as close as possible to the risk of the Bayes classifier?

Though we can't compute the Bayes classifier, we *can* count the number of misclassifications on the training data.  This quantity is called **empirical risk** and is defined as [@vonluxburgStatisticalLearningTheory2008]:

$$R_{emp}(f):=\frac{1}{n}\displaystyle\sum_{i=1}^n l(X_i, Y_i, f(X_i)).$$





While the loss function 
That is the setup. Now, we succinctly survey three concepts used to justify evaluations made on the basis of training data.      

1. Risk
2. Generalization 
3. Consistency   


## The Algorithms
### Linear Regression
Linear regression is a probabilistic model for linear data that assumes that each target value, $y_i$ is a function of one or more features $x_i$ and is generated by some unknown true function:

$$y = \theta_0+\displaystyle\sum_{i=1}^p\theta_ix_i$$.
Here our estimate of $y$, denoted by $\hat{y}$ is given by:

$$\hat{y} = \hat{\theta}_0+\displaystyle\sum_{i=1}^p\hat{\theta}_ix_i+\epsilon_i$$.
By convention and convenience, a 1 is included as a constant variable $x_0$ in the matrix $X$ formed by the vectors $x_i$, which allows us to express the linear model is a more compact form as follows:
$$\hat{y}=X^T \hat{\theta}+\epsilon$$.

The fact that our model doesn't fit the data perfectly is captured in the vector quantity $e$, noise that we assume is Gaussian: $\epsilon \sim \mathcal{N}(0,\sigma^2)$. We also assume that the true values of the parameters $\theta$, the error $\epsilon$, and its variance, $\sigma^2$, are all fixed (deterministic), but unknown. 

#### Estimating Model Parameters via Least Squares

Whether the parameters of this linear model can be solved analytically using calculus and a bit linear algebra, or numerically with calculus and a bit of linear algebra, depends upon properties of the data. If $X$ is full rank i.e. the columns are uncorrelated, the model parameters $\hat{\theta}$ may be estimated analytically by setting the derivative of the model equation to zero and solving for $\theta$. If, on the other hand, multicollinearity is extant, a numerical estimation method such as gradient descent must be used. In either case, parameter estimation is related to minimizing the **least squares** cost function.
$$J(\theta)=\frac{1}{2n}\displaystyle\sum_{i=1}^n(\theta x^{(i)}-y^{(i)})^2$$
where the superscript $^{(i)}$ denotes the observation number. To put this in matrix format, we have:
$$J(\theta)=\frac{1}{2n}(X\theta - y)^T(X\theta-y)$$
A bit of algebraic simplification yields:

$$J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty$$
Taking the derivative of both sides and setting it to zero, gives:

$$\frac{\partial J}{\partial \theta}=2X^TX\theta-2X^{T}y=0.$$
With a bit more manipulation, we have:
$$X^TX\theta=X^{T}y$$

If $X^TX$ is invertible, i.e. $X$ is full rank we can multiply both sides by $(X^TX)^{-1}$ to obtain the **normal equation**.
$$\theta=(X^TX)^{-1}X^Ty$$


